---
title: "HealthCostForecast"
author: "Nazmul Farooquee"
date: "`r format(Sys.Date(), '%Y-%m-%d')`"
output:
  html_document:
    toc: true
    toc_depth: 2
    number_sections: true
    theme: cosmo
    highlight: tango
  pdf_document:
    toc: true
    toc_depth: 2
  word_document:
    toc: true
    toc_depth: 2
---



**Project Overview**

The HealthCostForecast project aims to develop a sophisticated predictive model to estimate health insurance costs using various demographic and lifestyle factors. By leveraging a comprehensive dataset that includes age, gender, BMI, smoking status, number of dependents, and geographic region, this project will apply advanced statistical analysis and machine learning techniques to uncover the underlying patterns influencing insurance charges. Our goal is to deliver actionable insights that can inform stakeholders in the insurance and healthcare industries, as well as individual consumers, about the determinants of insurance costs.


1: Data Collection
Description

In this step, we will load the dataset into R and perform initial inspections to understand its structure. This includes:

    Importing necessary libraries: We will import libraries required for data manipulation and analysis.
    Loading the dataset from a CSV file: We will read the dataset into an R data frame.
    Displaying basic information about the dataset: We will inspect the dataset to understand its structure, including the types of variables and basic statistical summaries.



# 1. **Import Necessary Libraries:**

Description

In this step, we will load the dataset into R and perform initial inspections to understand its structure. This includes:

Importing necessary libraries: We will import libraries required for data manipulation and analysis.
Loading the dataset from a CSV file: We will read the dataset into an R data frame.
Displaying basic information about the dataset: We will inspect the dataset to understand its structure, including the types of variables and basic statistical summaries.

## 1.1 Import Necessary Libraries:

```{r}
# Importing necessary libraries
library(tidyverse)  # For data manipulation and visualization
library(data.table) # For efficient data handling

```

## 1.2 Load the Dataset:

```{r}
# Loading the dataset from a CSV file
df <- fread("insurance.csv")

```

## 1.3 Display Basic Information:

```{r}
# Displaying the structure of the dataset
str(df)

# Providing a statistical summary of the dataset
summary(df)

```

### Observations

- The dataset contains 1,338 observations and 7 variables.
- **Age**: The ages range from 18 to 64, with a median age of 39. The average age is 39.21 years.
- **Sex**: The `sex` variable is a character string indicating gender, which will be converted to a factor in the preprocessing step.
- **BMI**: The Body Mass Index (BMI) ranges from 15.96 to 53.13, with a mean of 30.66.
- **Children**: The number of children/dependents ranges from 0 to 5, with a mean of 1.095.
- **Smoker**: The `smoker` variable is a character string indicating smoking status, which will be converted to a factor in the preprocessing step.
- **Region**: The `region` variable is a character string representing the residential area in the US, which will also be converted to a factor.
- **Charges**: The medical costs billed by health insurance range from $1,122 to $63,770, with a mean of $13,270.


# 2. **Data Preprocessing**

## 2.1 Checking for Missing Values

**Description**  
The first step in data preprocessing is to check for missing values in the dataset. This ensures that there are no gaps in the data that could affect the analysis and model building.

```{r}
# Checking for missing values in the dataset
missing_values <- colSums(is.na(df))
missing_values

```

## 2.2 Encoding Categorical Variables

**Description**  
The next step in data preprocessing is to convert categorical variables into factors. This step is essential for statistical modeling and analysis in R, as it allows these variables to be properly handled in various analyses and visualizations.

```{r}
# Encoding categorical variables as factors
df$sex <- as.factor(df$sex)
df$smoker <- as.factor(df$smoker)
df$region <- as.factor(df$region)

```


## 2.3 Displaying the First Few Rows

**Description**  
The final step in the initial data preprocessing is to display the first few rows of the dataset. This allows us to verify that the categorical variables have been correctly encoded as factors and to gain a preliminary understanding of the data.

```{r}
# Displaying the first few rows of the dataset to confirm preprocessing
head(df)

```


# 3. **Exploratory Data Analysis (EDA)**

**Description**  
Exploratory Data Analysis (EDA) is a crucial step in understanding the underlying patterns and relationships within the dataset. This includes visualizing the distribution of variables, identifying correlations, and uncovering any anomalies or insights.

## 3.1 Visualizing the Distribution of Charges

**Description**  
We will start by visualizing the distribution of the `charges` variable to understand its spread and central tendency.

```{r}
# Visualizing the distribution of charges
ggplot(df, aes(x = charges)) +
  geom_histogram(binwidth = 1000, fill = "blue", color = "white") +
  theme_minimal() +
  labs(title = "Distribution of Charges")


```


**Outcome**  
The histogram shows the distribution of the charges variable.

**Observations**

- The distribution of charges is right-skewed, indicating that most individuals have lower medical costs, with fewer individuals having very high costs.
- The majority of the charges are clustered between $0 and $20,000, with a significant peak around $0 to $2,000.
- There are a few high-cost outliers above $50,000, indicating some individuals incur significantly higher medical expenses.




## 3.2 Visualizing the Relationship between Variables

**Description**  
Next, we will visualize the relationships between the `charges` variable and other variables such as `age`, `bmi`, and `smoker` status to identify any patterns.

```{r}

# Load necessary libraries
library(tidyverse)
library(data.table)
library(GGally)
library(ggplot2)
library(corrplot)

# Pair plot to visualize relationships
ggpairs(df, columns = 1:6, aes(color = smoker))

```

**Observations from the Pair Plot**

**Outcome**

The pair plot provides a matrix of scatter plots and box plots, showing relationships between pairs of variables such as age, sex, bmi, children, smoker, region, and charges. Here are the detailed observations:

1. **Age vs. Other Variables:**
    - **Age and Charges:** There's a noticeable positive correlation between age and charges. As age increases, the charges tend to increase as well.
    - **Age and BMI:** There is no strong correlation between age and BMI.

2. **Sex vs. Other Variables:**
    - **Sex and Charges:** The charges appear to be relatively evenly distributed across males and females.
    - **Sex and Smoker:** The distribution of smokers and non-smokers is quite balanced across both sexes.

3. **BMI vs. Other Variables:**
    - **BMI and Charges:** There is a moderate positive correlation between BMI and charges, especially for smokers. Higher BMI often correlates with higher medical charges.
    - **BMI and Smoker:** Smokers tend to have a higher BMI compared to non-smokers.

4. **Children vs. Other Variables:**
    - **Children and Charges:** There is no strong correlation between the number of children and charges. The presence of children does not significantly affect medical charges.
    - **Children and Region:** The number of children appears evenly distributed across different regions.

5. **Smoker vs. Other Variables:**
    - **Smoker and Charges:** Smokers generally incur significantly higher charges than non-smokers, indicating smoking is a strong factor in predicting medical costs.
    - **Smoker and Region:** The distribution of smokers varies slightly by region, but no strong regional patterns are apparent.

6. **Region vs. Other Variables:**
    - **Region and Charges:** The charges vary slightly by region, but the differences are not substantial.
    - **Region and Sex:** The sex distribution is fairly uniform across different regions.

# 4. **Model Development**

## 4.1 Splitting the Data

**Description:**

We will split the dataset into training and testing sets to evaluate the model's performance on unseen data. This is a crucial step to ensure that the model generalizes well to new data.


```{r}
# Set seed for reproducibility
set.seed(123)

# Create a random sample index for training set (70% of the data)
train_index <- sample(seq_len(nrow(df)), size = 0.7 * nrow(df))

# Split the data into training and testing sets
training_set <- df[train_index, ]
testing_set <- df[-train_index, ]

# Display the dimensions of the training and testing sets
dim(training_set)
dim(testing_set)

```


## 4.2 Developing a Linear Regression Model

**Description:**

In this step, we will develop a linear regression model using the training dataset. Linear regression will help us understand the relationship between the dependent variable (charges) and the independent variables (age, sex, bmi, children, smoker, region).


```{r}
# Develop the linear regression model using the training set
linear_model <- lm(charges ~ age + sex + bmi + children + smoker + region, data = training_set)

# Display the summary of the linear regression model
summary(linear_model)


```



**Observations from the Linear Regression Model Summary**

- **Intercept:** The intercept of -12473.54 indicates the baseline charges when all predictors are at zero.
- **Age:** The coefficient for age is 240.08, which means that for each additional year of age, the charges increase by approximately $240, holding other variables constant. This coefficient is highly significant (p < 2e-16).
- **Sex:** The coefficient for sex (male) is -248.87, indicating that being male reduces the charges by approximately $248 compared to females. However, this coefficient is not statistically significant (p = 0.5426).
- **BMI:** The coefficient for BMI is 369.94, suggesting that for each unit increase in BMI, the charges increase by approximately $370, holding other variables constant. This coefficient is highly significant (p < 2e-16).
- **Children:** The coefficient for the number of children is 673.35, indicating that each additional child increases the charges by approximately $673. This coefficient is also highly significant (p = 5.07e-05).
- **Smoker:** The coefficient for smokers is 23938.29, meaning that smokers incur significantly higher charges by approximately $23,938 compared to non-smokers. This coefficient is highly significant (p < 2e-16).
- **Region:** The coefficients for the regions indicate that, compared to the reference region (not specified in the output):
  - Northwest region: Charges decrease by approximately $595.16, but this is not statistically significant (p = 0.3056).
  - Southeast region: Charges decrease by approximately $948.20, also not statistically significant (p = 0.1061).
  - Southwest region: Charges decrease by approximately $1116.66, with a marginal significance (p = 0.0552).
- **Model Fit:** 
  - The Residual Standard Error (RSE) is 6185, indicating the average amount that the response (charges) deviate from the true regression line.
  - Multiple R-squared: 0.7478, meaning that approximately 74.78% of the variability in charges can be explained by the model.
  - Adjusted R-squared: 0.7456, which is slightly adjusted for the number of predictors.
  - The overall F-statistic is 343.6 on 8 and 927 degrees of freedom, with a p-value < 2.2e-16, indicating that the model is statistically significant.



## 4.3 Making Predictions and Evaluating the Model

**Description:**

In this step, we will use the linear regression model to make predictions on the testing dataset. Then, we will evaluate the model's performance using appropriate metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE).


```{r}
# Make predictions on the testing set
predictions <- predict(linear_model, newdata = testing_set)

# Calculate evaluation metrics
mae <- mean(abs(predictions - testing_set$charges))
mse <- mean((predictions - testing_set$charges)^2)
rmse <- sqrt(mse)

# Display the evaluation metrics
mae
mse
rmse


```
**Observations from Model Evaluation**

**Outcome:**

The model's performance is evaluated using three metrics: Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). Here are the observed values:

- **MAE**: 3993.986
- **MSE**: 33893854
- **RMSE**: 5821.843

**Observations:**

1. **Mean Absolute Error (MAE)**:
    - The MAE value is 3993.986, which means that on average, the model's predictions deviate from the actual charges by approximately \$3994. This gives a straightforward interpretation of the prediction errors in the same units as the charges.

2. **Mean Squared Error (MSE)**:
    - The MSE value is 33,893,854, which indicates the average of the squared differences between predicted and actual values. This metric penalizes larger errors more than smaller ones, making it useful for identifying models with significant outliers.

3. **Root Mean Squared Error (RMSE)**:
    - The RMSE value is 5821.843, which is the square root of the MSE. RMSE provides an error metric that is in the same units as the target variable (charges) and can be interpreted as the standard deviation of the residuals. An RMSE of 5821.843 indicates that the model's predictions deviate from the actual charges by about \$5822 on average.

**Summary:**

The observed error metrics (MAE, MSE, and RMSE) provide insights into the model's performance. The MAE value indicates that the average prediction error is about \$3994. The MSE and RMSE values suggest that there are some larger deviations in the predictions, likely due to outliers in the data. Overall, while the model captures the general trends, there is room for improvement to reduce these error values further.

With these observations in mind, we can proceed to further model tuning or explore other modeling techniques to enhance the prediction accuracy.


## 4.4.1 Visualizing the Residuals

**Description:**

In this step, we will visualize the residuals of the linear regression model to assess its performance. Residuals are the differences between the actual values and the predicted values. Visualizing residuals helps us to check for patterns that might indicate issues with the model, such as non-linearity, heteroscedasticity, or outliers.


```{r}
# Visualizing the residuals
residuals <- testing_set$charges - predictions

# Create a data frame for plotting
residuals_df <- data.frame(predictions, residuals)

# Plot residuals
ggplot(residuals_df, aes(x = predictions, y = residuals)) +
  geom_point(color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "Residuals vs. Predicted Values",
       x = "Predicted Values",
       y = "Residuals")

```

**Observations from Residual Plot**

**Outcome:**

The residual plot visualizes the residuals (differences between the actual and predicted values) against the predicted values. Here are the key observations:

1. **Pattern of Residuals:**
    - The residuals are not randomly dispersed around the horizontal axis (y = 0). Instead, there are patterns that suggest the model may not fully capture the relationship between the variables.
    - There appears to be a funnel shape, indicating potential heteroscedasticity (i.e., the variance of the residuals is not constant across all levels of the predicted values).

2. **Non-linearity:**
    - The residuals exhibit a curve pattern, suggesting non-linearity in the data that is not captured by the linear model. This implies that a more complex model might be necessary.

3. **Outliers:**
    - There are several residuals with large positive or negative values, indicating potential outliers in the data. These outliers may have a significant impact on the model's performance and should be investigated further.

**Summary:**

The residual plot indicates that the linear regression model has some issues with heteroscedasticity, non-linearity, and outliers. These findings suggest that the model's assumptions may not be fully met and that further model refinement or alternative modeling techniques may be necessary to improve predictive performance.


## 4.4.2 Improving the Model

Description:

In this step, we will attempt to improve the linear regression model by addressing the issues identified in the residual plot. We will introduce polynomial regression to handle non-linearity and use logarithmic transformation to address heteroscedasticity.

```{r}
# Load necessary libraries
library(tidyverse)

# Create polynomial features for non-linearity
training_set <- training_set %>%
  mutate(age2 = age^2, bmi2 = bmi^2, age_bmi = age * bmi)

testing_set <- testing_set %>%
  mutate(age2 = age^2, bmi2 = bmi^2, age_bmi = age * bmi)

# Fit a polynomial regression model
poly_model <- lm(charges ~ age + age2 + sex + bmi + bmi2 + age_bmi + children + smoker + region, data = training_set)

# Display the summary of the polynomial regression model
summary(poly_model)

# Predict on the testing set
predictions_poly <- predict(poly_model, newdata = testing_set)

# Calculate evaluation metrics for the polynomial model
mae_poly <- mean(abs(predictions_poly - testing_set$charges))
mse_poly <- mean((predictions_poly - testing_set$charges)^2)
rmse_poly <- sqrt(mse_poly)

# Display the evaluation metrics
mae_poly
mse_poly
rmse_poly


```

**Observations from Polynomial Regression Model**

#### Outcome:
The polynomial regression model includes additional polynomial terms for `age` and `bmi` to capture potential non-linear relationships. Here are the key observations from the model summary:

1. **Coefficients:**
    - **Intercept:** The intercept is -14922.396, which is the expected value of charges when all predictors are zero.
    - **Age:** The coefficient for `age` is -133.830 with a p-value of 0.269608, indicating that `age` is not statistically significant in this model.
    - **Age^2:** The coefficient for `age^2` is 4.729 with a p-value of 0.000258, indicating that the quadratic term for age is statistically significant.
    - **Sex (male):** The coefficient for `sexmale` is -296.344 with a p-value of 0.464893, indicating that sex is not statistically significant in this model.
    - **BMI:** The coefficient for `bmi` is 953.605 with a p-value of < 0.000195, indicating that `bmi` is statistically significant.
    - **BMI^2:** The coefficient for `bmi^2` is -9.243 with a p-value of 0.015964, indicating that the quadratic term for BMI is statistically significant.
    - **Age * BMI:** The interaction term for `age_bmi` has a coefficient of -0.103 with a p-value of 0.965725, indicating that this interaction is not statistically significant.
    - **Children:** The coefficient for `children` is 891.127 with a p-value of < 3.79e-07, indicating that the number of children is statistically significant.
    - **Smoker (yes):** The coefficient for `smokeryes` is 24031.054 with a p-value of < 2e-16, indicating that being a smoker is statistically significant and has a large positive impact on charges.
    - **Region:**
        - Northwest: Coefficient is -751.390 with a p-value of 0.192993.
        - Southeast: Coefficient is -931.070 with a p-value of 0.110057.
        - Southwest: Coefficient is -1165.724 with a p-value of 0.043747, indicating statistical significance.

2. **Model Fit:**
    - **Residual Standard Error:** The residual standard error is 6131 on 924 degrees of freedom.
    - **Multiple R-squared:** The multiple R-squared value is 0.753, indicating that approximately 75.3% of the variance in charges is explained by the model.
    - **Adjusted R-squared:** The adjusted R-squared value is 0.750, which adjusts for the number of predictors in the model.
    - **F-statistic:** The F-statistic is 256.1 with a p-value < 2.2e-16, indicating that the model is statistically significant.

#### Summary:
The polynomial regression model with additional polynomial terms for `age` and `bmi` improves the model fit compared to the linear regression model. Significant predictors include the quadratic terms for age and BMI, the number of children, and smoking status. However, some predictors such as the interaction term for age and BMI, and sex are not statistically significant. The adjusted R-squared value suggests that the model explains a substantial portion of the variance in medical charges. Further model refinement or alternative modeling techniques may be explored to improve the predictive performance.


## 4.5 Visualizing Polynomial Regression Residuals

#### Description:

In this step, we will visualize the residuals of the polynomial regression model to assess its performance. This helps us understand if the polynomial regression model addresses the issues identified earlier with the linear regression model's residuals, such as non-linearity, heteroscedasticity, or outliers.

```{r}
# Predict using the polynomial regression model
poly_predictions <- predict(poly_model, newdata = testing_set)

# Calculate residuals for the polynomial regression model
poly_residuals <- testing_set$charges - poly_predictions

# Create a dataframe for plotting
poly_residuals_df <- data.frame(predictions = poly_predictions, residuals = poly_residuals)

# Plot polynomial regression residuals
ggplot(poly_residuals_df, aes(x = predictions, y = residuals)) +
  geom_point(color = "blue") +
  geom_hline(yintercept = 0, linetype = "dashed", color = "red") +
  theme_minimal() +
  labs(title = "Polynomial Regression Residuals vs. Predicted Values",
       x = "Predicted Values",
       y = "Residuals")


```

**Observations from Polynomial Regression Residuals Plot**

#### Outcome:
The residual plot visualizes the residuals (differences between the actual and predicted values) against the predicted values for the polynomial regression model. Here are the key observations:

1. **Pattern of Residuals:**
   - The residuals still show a pattern, but there is some improvement compared to the linear regression residuals plot.
   - The funnel shape indicating heteroscedasticity is still present, suggesting that the variance of the residuals is not constant across all levels of predicted values.

2. **Non-linearity:**
   - The non-linearity is less pronounced compared to the linear regression model, indicating that the polynomial regression model captures some of the non-linear relationships in the data.

3. **Outliers:**
   - There are still several residuals with large positive or negative values, indicating potential outliers in the data. These outliers may have a significant impact on the modelâ€™s performance and should be investigated further.

#### Summary:
The polynomial regression model improves the fit compared to the linear regression model by addressing some of the non-linearity in the data. However, the presence of heteroscedasticity and outliers suggests that further model refinement or alternative modeling techniques may be necessary to improve predictive performance.



## 4.6 Comparing Model Performance

Description:

In this step, we will compare the performance of the linear regression model and the polynomial regression model using evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). This comparison helps us determine if the polynomial regression model offers significant improvements over the linear regression model.



```{r}
# Make predictions on the testing set using the linear model
linear_predictions <- predict(linear_model, newdata = testing_set)

# Calculate evaluation metrics for the linear model
linear_mae <- mean(abs(linear_predictions - testing_set$charges))
linear_mse <- mean((linear_predictions - testing_set$charges)^2)
linear_rmse <- sqrt(linear_mse)

# Display the evaluation metrics for the linear model
linear_mae
linear_mse
linear_rmse

# Make predictions on the testing set using the polynomial model
poly_predictions <- predict(poly_model, newdata = testing_set)

# Calculate evaluation metrics for the polynomial model
poly_mae <- mean(abs(poly_predictions - testing_set$charges))
poly_mse <- mean((poly_predictions - testing_set$charges)^2)
poly_rmse <- sqrt(poly_mse)

# Display the evaluation metrics for the polynomial model
poly_mae
poly_mse
poly_rmse

# Compare with the linear regression model metrics
linear_mae
linear_mse
linear_rmse


```


**Observations from Comparing Model Performance**

**Outcome:**
In this step, we compared the performance of the linear regression model and the polynomial regression model using evaluation metrics such as Mean Absolute Error (MAE), Mean Squared Error (MSE), and Root Mean Squared Error (RMSE). Here are the observed values:

**Linear Regression Model:**
- **MAE:** 3993.986
- **MSE:** 33893854
- **RMSE:** 5821.843

**Polynomial Regression Model:**
- **MAE:** 4102.157
- **MSE:** 34022026
- **RMSE:** 5832.84

#### Observations:
1. **Mean Absolute Error (MAE):**
   - The linear regression model has a slightly lower MAE (3993.986) compared to the polynomial regression model (4102.157). This indicates that, on average, the linear regression model's predictions are closer to the actual values compared to the polynomial model.

2. **Mean Squared Error (MSE):**
   - The linear regression model has a slightly lower MSE (33893854) compared to the polynomial regression model (34022026). This suggests that the overall squared differences between the predicted and actual values are marginally smaller for the linear regression model.

3. **Root Mean Squared Error (RMSE):**
   - The RMSE values for both models are very close, with the linear regression model having a slightly lower RMSE (5821.843) compared to the polynomial regression model (5832.84). This indicates that the overall prediction error in the same units as the target variable is slightly lower for the linear regression model.

#### Summary:
- The evaluation metrics (MAE, MSE, and RMSE) indicate that there is no significant improvement in using the polynomial regression model over the linear regression model. In fact, the linear regression model performs slightly better in terms of these error metrics.
- Despite introducing polynomial terms to capture non-linearity, the polynomial regression model does not provide a substantial improvement in predictive performance.
- Further model refinement or exploring other modeling techniques might be necessary to achieve better predictive performance.




# 5. **Conclusion**

#### Description:

In this final step, we summarize our findings from the regression analysis. We discuss the insights gained from comparing the performance of the linear and polynomial regression models, highlight the key takeaways, and suggest potential next steps for further improving the model or exploring other modeling techniques.

#### Key Findings:

1. **Model Performance:**
   - The linear regression model performed slightly better than the polynomial regression model based on the evaluation metrics (MAE, MSE, and RMSE). This suggests that adding polynomial terms did not significantly improve the predictive accuracy.

2. **Residual Analysis:**
   - Both models showed patterns in the residual plots, indicating potential issues such as non-linearity, heteroscedasticity, and outliers. The polynomial regression model showed some improvement, but these issues were not entirely resolved.

3. **Significant Predictors:**
   - Key predictors identified include age, BMI, number of children, and smoking status. The quadratic terms for age and BMI were statistically significant in the polynomial model, suggesting non-linear relationships.

#### Potential Next Steps:

1. **Model Refinement:**
   - Further refine the models by exploring other polynomial terms or interaction terms.
   - Consider using regularization techniques such as Ridge or Lasso regression to handle multicollinearity and improve model generalizability.

2. **Alternative Models:**
   - Explore other modeling techniques such as decision trees, random forests, or gradient boosting machines, which might capture non-linear relationships and interactions more effectively.

3. **Feature Engineering:**
   - Investigate additional feature engineering techniques, such as creating interaction terms, logarithmic transformations, or domain-specific features that might improve model performance.

4. **Data Collection:**
   - Collect more data or additional features that could provide more information and potentially improve the model's predictive power.

#### **Final Thoughts:**

Concluding this project, we have established a solid foundation in understanding the relationships between various factors and medical charges. While the linear regression model provided a slightly better fit compared to the polynomial model, there is room for further enhancement. By considering the suggested next steps, we can continue to improve the model's predictive performance and robustness, leading to more accurate insights and better decision-making.
